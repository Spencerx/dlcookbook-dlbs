{
  "parameters": {
    "tensorrt.launcher": {
      "val": "${DLBS_ROOT}/scripts/launchers/tensorrt.sh",
      "type": "str",
      "desc": "Path to script that launches TensorRT benchmarks."
    },
    "tensorrt.env": {
      "val": [
        "${runtime.EXPORT_CUDA_CACHE_PATH}",
        "${runtime.EXPORT_CUDA_VISIBLE_DEVICES}"
      ],
      "type": "str",
      "desc": "Environmental variables to set for TensorRT benchmarks."
    },
    "tensorrt.cache": { 
      "val": "${HOME}/.tensorrt_cache",
      "type": "str",
      "desc": [
        "A path to store calibration caches for TensorRT when running in INT8 inference regime.",
        "If empty, no cache will be used and calibration will be done with every benchmark what's",
        "not desireable for models that's input size is large. The advice is to run quick runs for",
        "every model with this path set what will result in cache being populated. Then run real benchmarks",
        "and provide same cache path."
      ]
    },
    "tensorrt.args": {
      "val": [
        "--model ${exp.model}",
        "--model_file ${tensorrt.model_dir}/${tensorrt.model_file}",
        "--batch_size ${exp.replica_batch}",
        "--dtype ${exp.dtype}",
        "--num_warmup_batches ${exp.num_warmup_batches}",
        "--num_batches ${exp.num_batches}",
        "$('--profile' if ${tensorrt.profile} is True else '')$",
        "--input ${tensorrt.input}",
        "--output ${tensorrt.output}",
        "--cache $('' if '${tensorrt.cache}' == '' else '${tensorrt.cache}' if ${exp.docker} is False else '/workspace/tensorrt_cache')$"
      ],
      "type": "str",
      "desc": "Command line arguments that launcher uses to launch TensorRT."
    },
    "tensorrt.model_file": {
      "val": "${exp.id}.model.prototxt",
      "type": "str",
      "desc": "Caffe's prototxt inference (deploy) model file."
    },
    "tensorrt.model_dir": {
      "val": "$('${DLBS_ROOT}/models/${exp.model}' if ${exp.docker} is False else '/workspace/model')$",
      "type": "str",
      "desc": "Directory where Caffe's model file is located. Different for host/docker benchmarks."
    },
    "tensorrt.docker_image": {
      "val": "hpe/tensorrt:cuda9-cudnn7",
      "type": "str",
      "desc": "The name of a docker image to use for TensorRT."
    },
    "tensorrt.docker_args": {
      "val": [
        "-i",
        "--security-opt seccomp=unconfined",
        "--pid=host",
        "--volume=${DLBS_ROOT}/models/${exp.model}:/workspace/model",
        "$('--volume=${runtime.cuda_cache}:/workspace/cuda_cache' if '${runtime.cuda_cache}' else '')$",
        "$('--volume=${tensorrt.cache}:/workspace/tensorrt_cache' if '${tensorrt.cache}' != '' else '')$",
        "$('--volume=${monitor.pid_folder}:/workspace/tmp' if ${monitor.frequency} > 0 else '')$",
        "${exp.docker_args}",
        "${exp.docker_image}"
      ],
      "type": "str",
      "desc": "In case if containerized benchmarks, this are the docker parameters."
    },
    "tensorrt.profile": {
      "val": false,
      "type": "bool",
      "desc": "If true, per layer statistics are measured."
    },
    "tensorrt.input": {
      "val": "data",
      "type": "str",
      "desc": "Name of an input data tensor (data)"
    },
    "tensorrt.output": {
      "val": "prob",
      "type": "str",
      "desc": "Name of an output tensor (prob)"
    },
    "tensorrt.host_path": {
      "val": "${DLBS_ROOT}/src/tensorrt/build",
      "type": "str",
      "desc": "Path to a tensorrt executable in case of bare metal run."
    },
    "tensorrt.host_libpath": {
      "val": "",
      "type": "str",
      "desc": "Basically, it's a LD_LIBRARY_PATH for TensorRT in case of a bare metal run (should be empty)."
    },
    "tensorrt.data_dir": {
      "val": "",
      "type": "str",
      "desc": [
        "*** This parameter is not used since TensorRT backend does not support real data. ***",
        "*** It is here for compatibility reasons with exp.data_dir ***"
      ]
    }
  },
  "extensions": [
    {
      "condition":{ "exp.framework": "tensorrt" },
      "parameters": {
        "exp.framework_title": "TensorRT",
        "exp.status": "$('disabled' if ('${exp.phase}'=='training') or ('${exp.device_type}'=='cpu') or (${exp.num_local_gpus}>1) else 'ok')$"
      }
    },
    {
      "condition":{ "exp.framework": "tensorrt", "exp.env": "host" },
      "parameters": { "tensorrt.env": [
        "PATH=$('${tensorrt.host_path}:\\$PATH'.strip(' \t:'))$",
        "LD_LIBRARY_PATH=$('${tensorrt.host_libpath}:\\$LD_LIBRARY_PATH'.strip(' \t:'))$",
        "${runtime.EXPORT_CUDA_CACHE_PATH}",
        "${runtime.EXPORT_CUDA_VISIBLE_DEVICES}"
      ]}
    }
  ]
}
